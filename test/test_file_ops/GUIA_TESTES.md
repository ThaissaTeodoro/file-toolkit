# Guia de Testes - normalization_utils

Este guia explica como executar e interpretar os testes da biblioteca `normalization_utils`.

## ğŸ“ Estrutura dos Arquivos

```
normalization_utils/
â”œâ”€â”€ normalization_utils.py                 # Biblioteca principal
â”œâ”€â”€ test_normalization_utils.py            # Testes unitÃ¡rios e de integraÃ§Ã£o
â”œâ”€â”€ test_normalization_utils_performance.py # Testes de performance (opcional)
â”œâ”€â”€ conftest.py                     # ConfiguraÃ§Ã£o pytest (SparkSession, fixtures)
â”œâ”€â”€ pytest.ini                      # ConfiguraÃ§Ã£o do pytest
â”œâ”€â”€ test-requirements.txt           # DependÃªncias para testes
â”œâ”€â”€ run_tests.py                    # Script Python para facilitar execuÃ§Ã£o
â”œâ”€â”€ Makefile                        # Comandos automatizados (lint, test, cov, etc)
â””â”€â”€ GUIA_TESTES.md 
```

## ğŸš€ ExecuÃ§Ã£o RÃ¡pida

### OpÃ§Ã£o 1: Usando Makefile (Recomendado)
```bash
# Instalar dependÃªncias
make install

# Executar todos os testes
make test

# Executar com cobertura de cÃ³digo
make test-cov

# Executar testes em paralelo
make test-parallel
```

### OpÃ§Ã£o 2: Usando o script Python
```bash
# Instalar dependÃªncias e executar testes
python run_tests.py --install-deps --coverage

# Executar apenas testes rÃ¡pidos
python run_tests.py --markers "not slow"
```

### OpÃ§Ã£o 3: Usando pytest diretamente
```bash
# Instalar dependÃªncias
pip install -r test-requirements.txt

# Executar testes bÃ¡sicos
pytest test_normalization_utils.py -v

# Executar com cobertura
pytest test_normalization_utils.py --cov=json_utils --cov-report=html -v
```

## ğŸ“Š Tipos de Testes

### 1. Testes UnitÃ¡rios
Testam funÃ§Ãµes individuais isoladamente:
```bash
# Executar apenas testes unitÃ¡rios
make test-unit
# ou
pytest -m "unit" -v
```

**Cobertura:**
- âœ… `normalize_strings()`
- âœ… `normalize_column_names()` 
- âœ… `safe_string_to_double_spark()` 
- âœ… `get_logger()`

### 2. Testes de IntegraÃ§Ã£o
Testam fluxos completos combinando mÃºltiplas funÃ§Ãµes:
```bash
# Executar apenas testes de integraÃ§Ã£o
make test-integration
# ou
pytest -m "integration" -v
```

**CenÃ¡rios testados:**
- NormalizaÃ§Ã£o + conversÃ£o em pipelines
- DataFrames com mÃºltiplos tipos de dados

### 3. Testes de Performance
Verificam performance e escalabilidade:
```bash
# Executar testes de performance (podem demorar)
pytest test_normalization_utils_performance.py -v

# Pular testes lentos
pytest -m "not slow" -v
```

**MÃ©tricas avaliadas:**
- â±ï¸ Tempo de execuÃ§Ã£o para datasets grandes (1000+ registros)
- ğŸ”„ Throughput (registros/segundo)
- ğŸ’¾ Uso de memÃ³ria
- ğŸ“ˆ Escalabilidade com diferentes tamanhos de dados

## ğŸ·ï¸ Marcadores (Markers)
Os testes usam marcadores para categorizaÃ§Ã£o:

| Marcador | DescriÃ§Ã£o | Exemplo de Uso |
|----------|-----------|----------------|
| `unit` | Testes unitÃ¡rios | `pytest -m unit` |
| `integration` | Testes de integraÃ§Ã£o | `pytest -m integration` |
| `slow` | Testes que demoram (>5s) | `pytest -m "not slow"` |
| `spark` | Testes que usam SparkSession | `pytest -m spark` |
| `performance` | Testes de performance | `pytest -m performance` |
| `stress` | Testes de stress (muito pesados) | `pytest -m stress` |

## ğŸ“ˆ RelatÃ³rios de Cobertura

### Visualizar Cobertura HTML
```bash
make test-cov
# Abrir htmlcov/index.html no navegador
```

### Meta de Cobertura
- **Atual:** 95%+ 
- **MÃ­nimo aceitÃ¡vel:** 80%
- **Arquivos cobertos:** `normalization_utils.py`

## ğŸ”§ CenÃ¡rios de Teste EspecÃ­ficos

### Testes de Edge Cases
```bash
# Testar comportamento com dados problemÃ¡ticos
pytest test_normalization_utils.py::TestEdgeCases -v
```

**Casos cobertos:**
- Colunas inexistentes
- Valores nulos/vazios
- Colunas nÃ£o-string
- DataFrames sem colunas

### Testes de Tipos de Dados
```bash
# Testar conversÃµes de tipos
pytest test_normalization_utils.py::TestSafeStringToDoubleSpark::test_various_formats -v
```

**Tipos testados:**
- `strings` com nÃºmero em diferentes formatos
- `strings` com texto, vÃ­rgula, ponto, sÃ­mbolo, etc

### Testes de Performance por Tamanho
```bash
# Testar escalabilidade
pytest test_normalization_utils_performance.py::TestScalability -v
```

**CenÃ¡rios de escalabilidade:**
- 100, 500, 1000 registros
- 2, 3, 4 nÃ­veis de aninhamento
- Throughput mÃ­nimo: 50 registros/segundo

## ğŸ› Debugging e Troubleshooting

### Executar em Modo Debug
```bash
# Debug com breakpoints
make test-debug
# ou
pytest --pdb -v

# Executar teste especÃ­fico em debug
pytest test_json_utils.py::TestExtractJsonFields::test_extract_basic_fields --pdb -v
```

### Logs Detalhados
```bash
# Ver logs durante execuÃ§Ã£o
pytest --log-cli-level=DEBUG -s -v

# Capturar saÃ­da completa
pytest --capture=no -v
```

### Problemas Comuns

#### 1. SparkSession nÃ£o inicializa
**Erro:** `Exception: Could not find valid SPARK_HOME`
**SoluÃ§Ã£o:**
```bash
# Instalar PySpark localmente
pip install pyspark

# Ou definir SPARK_HOME
export SPARK_HOME=/path/to/spark
```

#### 2. Testes lentos demais
**Erro:** Testes demoram muito para executar
**SoluÃ§Ã£o:**
```bash
# Pular testes lentos
pytest -m "not slow" -v

# Executar em paralelo
pytest -n auto -v
```

#### 3. Problemas de memÃ³ria
**Erro:** `java.lang.OutOfMemoryError`
**SoluÃ§Ã£o:**
```bash
# Aumentar memÃ³ria do Spark
export SPARK_DRIVER_MEMORY=2g
export SPARK_EXECUTOR_MEMORY=2g
```

#### 4. Falhas intermitentes
**Erro:** Testes passam/falham aleatoriamente
**SoluÃ§Ã£o:**
```bash
# Executar mÃºltiplas vezes
pytest --count=3 -v

# Verificar concorrÃªncia
pytest -x -v  # Para no primeiro erro
```

## ğŸ“Š Interpretando Resultados

### Output Normal de Sucesso
```
========================= test session starts =========================
test_json_utils.py::TestExtractJsonFields::test_extract_basic_fields PASSED [12%]
test_json_utils.py::TestFlattenJsonColumns::test_flatten_nested_struct PASSED [25%]
...
========================= 48 passed in 12.34s =========================

Name                 Stmts   Miss  Cover   Missing
--------------------------------------------------
json_utils.py          156      8    95%   23-24, 87, 142-145
--------------------------------------------------
TOTAL                  156      8    95%
```

### MÃ©tricas de Performance Esperadas
```
ExtraÃ§Ã£o de 1000 registros: 5.23s
Throughput: 191 rec/s âœ… (> 50 rec/s)
Uso de memÃ³ria - Inicial: 245.2MB, Final: 267.8MB
Incremento: 22.6MB âœ… (< 200MB)
```

### Sinais de Alerta
âŒ **Cobertura < 80%** - Adicionar mais testes
âŒ **Throughput < 50 rec/s** - Otimizar performance
âŒ **Incremento memÃ³ria > 200MB** - PossÃ­vel vazamento
âŒ **Tempo > 30s para 1000 registros** - Performance degradada

## ğŸš€ CI/CD Integration

### GitHub Actions
```yaml
# .github/workflows/tests.yml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Run tests
        run: make test-ci
```

### Pipeline Completa
```bash
# Executar pipeline completa (lint + format + test + coverage)
make quality-check
```

**Pipeline inclui:**
1. âœ… Linting com flake8
2. âœ… FormataÃ§Ã£o com black
3. âœ… Testes unitÃ¡rios e integraÃ§Ã£o
4. âœ… Cobertura de cÃ³digo (>80%)
5. âœ… RelatÃ³rios HTML

## ğŸ“ Adicionando Novos Testes

### Template para Novo Teste
```python
def test_nova_funcionalidade(self, spark, sample_data):
    """Testa nova funcionalidade especÃ­fica."""
    # Arrange - Preparar dados
    df = spark.createDataFrame(sample_data, ["json_data"])
    expected_result = {...}
    
    # Act - Executar funÃ§Ã£o
    result = nova_funcao(df, parametros)
    
    # Assert - Verificar resultado
    assert result.count() == expected_count
    assert result.collect()[0]["campo"] == expected_value
```

### Checklist para Novos Testes
- [ ] Nome descritivo (`test_funcao_cenario`)
- [ ] Docstring explicando o teste
- [ ] Dados de entrada vÃ¡lidos
- [ ] VerificaÃ§Ã£o de resultado esperado
- [ ] Tratamento de edge cases
- [ ] Marcadores apropriados
- [ ] Performance aceitÃ¡vel

## ğŸ”„ ExecuÃ§Ã£o ContÃ­nua

### Watch Mode (Desenvolvimento)
```bash
# Reexecutar testes quando arquivos mudarem
make test-watch
# ou 
pytest --looponfail
```

### Testes EspecÃ­ficos Durante Desenvolvimento
```bash
# Testar apenas funÃ§Ã£o especÃ­fica
pytest -k "extract_json_fields" -v

# Testar classe especÃ­fica
pytest test_json_utils.py::TestExtractJsonFields -v

# Testar mÃ©todo especÃ­fico
pytest test_json_utils.py::TestExtractJsonFields::test_extract_basic_fields -v
```

## ğŸ“ Suporte

### Logs de Debug
Se encontrar problemas, execute com logs detalhados:
```bash
pytest --log-cli-level=DEBUG --tb=long -v > test_debug.log 2>&1
```

### InformaÃ§Ãµes do Ambiente
```bash
# VersÃµes instaladas
pip list | grep -E "(pyspark|pytest)"

# ConfiguraÃ§Ã£o do Spark
python -c "from pyspark.sql import SparkSession; print(SparkSession.builder.getOrCreate().version)"
```

### Limpeza Completa
```bash
# Limpar todos os caches e arquivos temporÃ¡rios
make clean

# Reinstalar dependÃªncias
pip uninstall -y pyspark pytest
pip install -r test-requirements.txt
```

---

## ğŸ¯ Resumo dos Comandos Principais

| AÃ§Ã£o | Comando |
|------|---------|
| **Setup inicial** | `make install` |
| **Testes bÃ¡sicos** | `make test` |
| **Com cobertura** | `make test-cov` |
| **Apenas rÃ¡pidos** | `make test-fast` |
| **Pipeline completa** | `make quality-check` |
| **Debug** | `make test-debug` |
| **Limpeza** | `make clean` |

**ğŸ‰ Pronto! Agora vocÃª tem uma suÃ­te de testes completa para sua biblioteca json_utils.**